{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This file manages the data input flow from two different input directory**"
      ],
      "metadata": {
        "id": "FUag3Llz8KP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "psVODpo48SDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "62rgoOuW8LUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Pythorch dataset: synchronize and load data from two different input modalities (spectrograms and waveforms) that correspond to the same sample, preparing them for a multi-stream neural network."
      ],
      "metadata": {
        "id": "YPA0L4hj_0Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "    def __init__(self, spec_root, wave_root, spec_transform=None, wave_transform=None):\n",
        "\n",
        "        self.spec_dataset = datasets.ImageFolder(spec_root)\n",
        "        self.wave_dataset = datasets.ImageFolder(wave_root)\n",
        "\n",
        "        self.spec_paths = self.spec_dataset.samples\n",
        "        self.wave_paths = self.wave_dataset.samples\n",
        "\n",
        "        self.spec_transform = spec_transform\n",
        "        self.wave_transform = wave_transform\n",
        "        self.class_names = self.spec_dataset.classes\n",
        "\n",
        "        if len(self.spec_paths) != len(self.wave_paths):\n",
        "             print(f\"error: Spectrograms found: {len(self.spec_paths)}, Waveform trovate: {len(self.wave_paths)}\")\n",
        "             raise AssertionError(\"Datasets have to have the same number of elements and in the same order\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spec_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec_path, label = self.spec_paths[idx]\n",
        "        wave_path, _ = self.wave_paths[idx]\n",
        "\n",
        "        spec_img = Image.open(spec_path).convert('RGB')\n",
        "        wave_img = Image.open(wave_path).convert('RGB')\n",
        "\n",
        "        if self.spec_transform:\n",
        "            spec_img = self.spec_transform(spec_img)\n",
        "        if self.wave_transform:\n",
        "            wave_img = self.wave_transform(wave_img)\n",
        "\n",
        "        return spec_img, wave_img, label"
      ],
      "metadata": {
        "id": "1r5hW1yd_bKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function serves as the data preparation: it utilizes the custom multimodal dataset to load data from the two distinct directories and organizes it into training, validation and testing components."
      ],
      "metadata": {
        "id": "ic9rVJnFApcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloaders(spec_train_dir, wave_train_dir, spec_test_dir, wave_test_dir,\n",
        "                    spec_transforms, wave_transforms, batch_size, val_split_ratio=0.20):\n",
        "\n",
        "    full_train_dataset = MultiModalDataset(\n",
        "        spec_root=spec_train_dir, wave_root=wave_train_dir,\n",
        "        spec_transform=spec_transforms, wave_transform=wave_transforms\n",
        "    )\n",
        "\n",
        "    #test dataset\n",
        "    test_dataset = MultiModalDataset(\n",
        "        spec_root=spec_test_dir, wave_root=wave_test_dir,\n",
        "        spec_transform=spec_transforms, wave_transform=wave_transforms\n",
        "    )\n",
        "\n",
        "    # Split Train/Validation\n",
        "    val_size = int(val_split_ratio * len(full_train_dataset))\n",
        "    train_size = len(full_train_dataset) - val_size\n",
        "    train_dataset, validation_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    class_names = full_train_dataset.class_names\n",
        "\n",
        "    return train_loader, validation_loader, test_loader, class_names"
      ],
      "metadata": {
        "id": "hjoDSjTuAj5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}